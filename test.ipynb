{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing, torch, re\n",
    "import os.path as osp\n",
    "import datasets\n",
    "\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig,\\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, BertForMaskedLM, FillMaskPipeline\n",
    "from tokenization_kobert import KoBertTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from time import strftime, time, localtime\n",
    "from os import listdir\n",
    "from functools import partial\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# model_path = \"skt/kogpt2-base-v2\"\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path, \n",
    "#                                                     bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "#                                                     pad_token='<pad>', mask_token='<mask>', max_length=512)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "# model = BertForMaskedLM.from_pretrained(\"skt/kobert-base-v1\")\n",
    "# model = AutoModel.from_pretrained('monologg/distilkobert')\n",
    "model = BertForMaskedLM.from_pretrained(\"monologg/kobert-lm\")\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/distilkobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=8002, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KoBertTokenizer(name_or_path='monologg/distilkobert', vocab_size=8002, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"안녕하세요.[MASK][MASK][MASK][MASK][MASK][MASK]\"\n",
    "\n",
    "# collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "encoded = tokenizer(sent, return_tensors=\"pt\")\n",
    "# target = tokenizer(\"안녕하세요\", return_tensors='pt')\n",
    "\n",
    "out = model(**encoded)\n",
    "pipe = FillMaskPipeline(model=model, tokenizer=tokenizer)\n",
    "# while True:\n",
    "#     t = model(**encoded)\n",
    "#     logits:torch.Tensor = t.logits\n",
    "#     predicted_token = logits[:,-1,:].argmax(dim=1)\n",
    "#     print(tokenizer.decode(predicted_token))\n",
    "\n",
    "# print(tokenizer.decode(logits[:,-1,:].argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.6325711607933044,\n",
       "   'token': 6749,\n",
       "   'token_str': '십',\n",
       "   'sequence': '[CLS] 안녕하십[MASK][SEP]'},\n",
       "  {'score': 0.10633417218923569,\n",
       "   'token': 6586,\n",
       "   'token_str': '세요',\n",
       "   'sequence': '[CLS] 안녕하세요[MASK][SEP]'},\n",
       "  {'score': 0.05746186524629593,\n",
       "   'token': 6579,\n",
       "   'token_str': '세',\n",
       "   'sequence': '[CLS] 안녕하세[MASK][SEP]'},\n",
       "  {'score': 0.0350675955414772,\n",
       "   'token': 7275,\n",
       "   'token_str': '죠',\n",
       "   'sequence': '[CLS] 안녕하죠[MASK][SEP]'},\n",
       "  {'score': 0.022893358021974564,\n",
       "   'token': 6601,\n",
       "   'token_str': '셔',\n",
       "   'sequence': '[CLS] 안녕하셔[MASK][SEP]'}],\n",
       " [{'score': 0.7970646619796753,\n",
       "   'token': 54,\n",
       "   'token_str': '.',\n",
       "   'sequence': '[CLS] 안녕하[MASK].[SEP]'},\n",
       "  {'score': 0.12322165817022324,\n",
       "   'token': 258,\n",
       "   'token_str': '?',\n",
       "   'sequence': '[CLS] 안녕하[MASK]?[SEP]'},\n",
       "  {'score': 0.03613218292593956,\n",
       "   'token': 5771,\n",
       "   'token_str': '니까',\n",
       "   'sequence': '[CLS] 안녕하[MASK]니까[SEP]'},\n",
       "  {'score': 0.012106643058359623,\n",
       "   'token': 6999,\n",
       "   'token_str': '요',\n",
       "   'sequence': '[CLS] 안녕하[MASK]요[SEP]'},\n",
       "  {'score': 0.004243792966008186,\n",
       "   'token': 5,\n",
       "   'token_str': '!',\n",
       "   'sequence': '[CLS] 안녕하[MASK]![SEP]'}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"안녕하[MASK][MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False,  True,  True, False]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_idxs = encoded['input_ids'] == tokenizer.mask_token_id\n",
    "mask_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 8002])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6749,   54])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = out.logits[mask_idxs].argmax(dim=-1)\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'십.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'요'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token = out.logits[0,-1].argmax(-1)\n",
    "tokenizer.decode(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455ed48350f142019bf9f45908cde56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/98652 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6cd95896b7a84724\n",
      "Found cached dataset text (C:/Users/sizzf/.cache/huggingface/datasets/text/default-6cd95896b7a84724/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc5f90129a14fdf8a2a57d9889b8af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n",
    "# dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "# dataset = dataset.remove_columns([col for col in dataset.column_names if col != 'text'])\n",
    "# dataset = load_dataset(\"bookcorpus\", split=\"train\", streaming=True).with_format('torch')\n",
    "# dataset = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "dataset = load_dataset(\"text\", data_dir=\"data\")\n",
    "# len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1657394\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[\"train\"].train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1160175\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 497219\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 : 맞아'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = dataset['train'][0]['text']\n",
    "s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'맞아'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dir(): return f\"tokenizer_{strftime('%m-%d-%H-%M', localtime(time()))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_iterator(batch_size=10000):\n",
    "#     for i in tqdm(range(0, len(dataset['train']), batch_size)):\n",
    "#         yield dataset['train'][i:i+batch_size]['text']\n",
    "# if [_dir for _dir in listdir() if \"tokenizer\" in _dir] != []:\n",
    "#     latest_tokenizer_path = sorted([_dir for _dir in listdir() if \"tokenizer\" in _dir])[-1]\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(latest_tokenizer_path)\n",
    "# else:\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\").train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=32_000)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"skplanet/dialog-koelectra-small-discriminator\").train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=32_000)\n",
    "#     tokenizer.save_pretrained(get_time_dir())\n",
    "\n",
    "# tokenizer.pad_token = '<pad>'\n",
    "# tokenizer.eos_token = '</s>'\n",
    "# tokenizer.bos_token = '</s>'\n",
    "# tokenizer.unk_token = '<unk>'\n",
    "# tokenizer.mask_token = '<mask>'\n",
    "# tokenizer.model_max_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 512\n"
     ]
    }
   ],
   "source": [
    "num_proc = multiprocessing.cpu_count()\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3135, 5724, 7814]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"안녕하세요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples:datasets.arrow_dataset.Batch, tokenizer=tokenizer):\n",
    "    print(type(examples))\n",
    "    print(examples['text'])\n",
    "    \n",
    "    for i, s in enumerate(examples['text']):\n",
    "        for match in reversed([*re.finditer(\"^[0-9]* : \", s)]):\n",
    "            s = s[:match.start()] + s[match.end():]\n",
    "        examples['text'][i] =s\n",
    "    \n",
    "    label:str = \"\"\n",
    "    tokenized_label = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(label))\n",
    "    \n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['text'], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length, padding=True\n",
    "    )\n",
    "    return tokenized_inputs|{\"label\":tokenized_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-350f673cbd97f766\n",
      "Found cached dataset text (C:/Users/sizzf/.cache/huggingface/datasets/text/default-350f673cbd97f766/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b222ddac54484d8ada565808da62be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = load_dataset(\"text\", data_files=[\"data/BAND_11_05.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cb265ed5924a29bbb57ebf7b9b8f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Batch'>\n",
      "['1 : 야 니 동생 이번에 나온다고 하더니!', '2 : 맞아 근데 코로나 때문에 잘렸어!', '1 : 역시 코로나 갑 오브 갑이네 키키', '2 : 그러게 하지만 금방 전역한대', '1 : 헐 벌써 시간이 그렇게 됐나?', '2 : 내 말이 아주 두려워 죽겠어 키키', '1 : 근데 원래 걔 휴가 날이 언제였지?', '2 : 음 잠시만 8월 중순 쯤이라 하던데?', '1 : 그래서 사람들한테 7월 말에 전화 돌렸구나?', '2 : 어 맞아 키키 아주 오지랖이 넓어', '1 : 그러니까 거기서 약술도 팔고 말이야 키키', '2 : 아빠 닮아서 장사꾼 다됐어 이런!', '1 : 그래도 잘 지내고 있네!', '2 : 맞아 살도 뒤룩뒤룩 쪘어 ㅠㅠ 으이구!', '1 : 야 걔는 운동하러 가서 왜 살이 쪘냐 ㅜㅜ', '2 : 그래도 운동 열심히 한대', '2 : 곧 나오니깐 같이 한번 보자!']\n",
      "1 : \n",
      "야 니 동생 이번에 나온다고 하더니!\n",
      "2 : \n",
      "맞아 근데 코로나 때문에 잘렸어!\n",
      "1 : \n",
      "역시 코로나 갑 오브 갑이네 키키\n",
      "2 : \n",
      "그러게 하지만 금방 전역한대\n",
      "1 : \n",
      "헐 벌써 시간이 그렇게 됐나?\n",
      "2 : \n",
      "내 말이 아주 두려워 죽겠어 키키\n",
      "1 : \n",
      "근데 원래 걔 휴가 날이 언제였지?\n",
      "2 : \n",
      "음 잠시만 8월 중순 쯤이라 하던데?\n",
      "1 : \n",
      "그래서 사람들한테 7월 말에 전화 돌렸구나?\n",
      "2 : \n",
      "어 맞아 키키 아주 오지랖이 넓어\n",
      "1 : \n",
      "그러니까 거기서 약술도 팔고 말이야 키키\n",
      "2 : \n",
      "아빠 닮아서 장사꾼 다됐어 이런!\n",
      "1 : \n",
      "그래도 잘 지내고 있네!\n",
      "2 : \n",
      "맞아 살도 뒤룩뒤룩 쪘어 ㅠㅠ 으이구!\n",
      "1 : \n",
      "야 걔는 운동하러 가서 왜 살이 쪘냐 ㅜㅜ\n",
      "2 : \n",
      "그래도 운동 열심히 한대\n",
      "2 : \n",
      "곧 나오니깐 같이 한번 보자!\n"
     ]
    }
   ],
   "source": [
    "# tokenized_dataset = dataset.map(partial(group_texts, tokenizer=tokenizer), batched=True, remove_columns=['text'], num_proc=num_proc)\n",
    "# tokenized_dataset = dataset.map(partial(group_texts, tokenizer=tokenizer), batched=True, remove_columns=['text'])\n",
    "# tokenized_dataset = dataset.map(partial(group_texts, tokenizer=tokenizer), batched=True, remove_columns=['text'])\n",
    "mapped = samples.map(partial(group_texts, tokenizer=tokenizer), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text'],\n",
       "         num_rows: 17\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text', 'input_ids', 'token_type_ids', 'special_tokens_mask', 'attention_mask'],\n",
       "         num_rows: 17\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples, mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1160175\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 497219\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size = len(tokenizer),\n",
    "    n_ctx = tokenizer.model_max_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=f\"AutocompleteLM-{get_time_dir()}\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    # num_train_epochs=1,\n",
    "    weight_decay=.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    max_steps=74004228 * (num_train_epochs:=1),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"안녕하세요, \"\n",
    "tokens = tokenizer(s, return_tensors=\"pt\")\n",
    "output = model.generate(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'].shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
