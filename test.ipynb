{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu113\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing, torch, re\n",
    "import os.path as osp\n",
    "import datasets\n",
    "\n",
    "from transformers import EncoderDecoderModel, BertTokenizerFast,\\\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments, BitsAndBytesConfig, AutoModel, AutoTokenizer, T5ForConditionalGeneration, StoppingCriteria, StoppingCriteriaList\n",
    "from tokenization_kobert import KoBertTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load as load_metric\n",
    "from tqdm.auto import tqdm\n",
    "from time import strftime, time, localtime\n",
    "from os import listdir\n",
    "from functools import partial\n",
    "from typing import List\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from utils import rouge, load_dataset\n",
    "\n",
    "torch.cuda.set_device(\"cuda:0\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"skt/kogpt2-base-v2\"\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path, \n",
    "#                                                     bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "#                                                     pad_token='<pad>', mask_token='<mask>', max_length=512)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "# model = BertForMaskedLM.from_pretrained(\"skt/kobert-base-v1\")\n",
    "# model = AutoModel.from_pretrained('monologg/distilkobert')\n",
    "# model = BertForMaskedLM.from_pretrained(\"monologg/kobert-lm\")\n",
    "# tokenizer = KoBertTokenizer.from_pretrained('monologg/distilkobert')\n",
    "quant8_cfg = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "quant4_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# path = \"output-12-16-11-03/checkpoint-16000\"\n",
    "path = \"output-12-16-01-58/checkpoint-18000\"\n",
    "\n",
    "# model = AutoModel.from_pretrained(path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(path)\n",
    "# model_8 = EncoderDecoderModel.from_pretrained(\"output/checkpoint-72000\", quantization_config=quant8_cfg)\n",
    "# model_4 = EncoderDecoderModel.from_pretrained(\"output/checkpoint-72000\", quantization_config=quant4_cfg)\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_idxs = encoded['input_ids'] == tokenizer.mask_token_id\n",
    "# print(mask_idxs)\n",
    "# print(out.logits.shape)\n",
    "# masks = out.logits[mask_idxs].argmax(dim=-1)\n",
    "# print(masks)\n",
    "# print(tokenizer.decode(masks))\n",
    "# predicted_token = out.logits[0,-1].argmax(-1)\n",
    "# tokenizer.decode(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n",
    "# dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "# dataset = dataset.remove_columns([col for col in dataset.column_names if col != 'text'])\n",
    "# dataset = load_dataset(\"bookcorpus\", split=\"train\", streaming=True).with_format('torch')\n",
    "# dataset = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_iterator(batch_size=10000):\n",
    "#     for i in tqdm(range(0, len(dataset['train']), batch_size)):\n",
    "#         yield dataset['train'][i:i+batch_size]['text']\n",
    "# if [_dir for _dir in listdir() if \"tokenizer\" in _dir] != []:\n",
    "#     latest_tokenizer_path = sorted([_dir for _dir in listdir() if \"tokenizer\" in _dir])[-1]\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(latest_tokenizer_path)\n",
    "# else:\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\").train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=32_000)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"skplanet/dialog-koelectra-small-discriminator\").train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=32_000)\n",
    "#     tokenizer.save_pretrained(get_time_dir())\n",
    "\n",
    "# tokenizer.pad_token = '<pad>'\n",
    "# tokenizer.eos_token = '</s>'\n",
    "# tokenizer.bos_token = '</s>'\n",
    "# tokenizer.mask_token = '<mask>'\n",
    "# tokenizer.model_max_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "\n",
    "    def __init__(self, stops = []):\n",
    "      StoppingCriteria.__init__(self), \n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, stops = []):\n",
    "      self.stops = stops\n",
    "      for i in range(len(stops)):\n",
    "        self.stops = self.stops[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dir(): return f\"{strftime('%m-%d-%H-%M', localtime(time()))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_iter_train_test\n",
    "# dataset = load_dataset(tokenizer=tokenizer)\n",
    "trainset, testset = load_iter_train_test(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one = dataset['train'][0]\n",
    "# one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "# model.config.eos_token_id = tokenizer.eos_token_id\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.config.max_new_tokens = 32\n",
    "# model.config.early_stopping = True\n",
    "# model.config.no_repeat_ngram_size = 1\n",
    "# model.config.length_penalty = 2.\n",
    "# model.config.repetition_penalty = 3.\n",
    "# model.config.num_beams = 10\n",
    "# model.config.vocab_size = model.config.encoder.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 32000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id, tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops = [tokenizer.eos_token_id, tokenizer.bos_token_id])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"summarize: \"+\"진짜 타이트하게 살고 딨\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded = tokenizer(sent, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[8449,  164,  156,  156,  144,  161,  152,  169,  148,    2, 5189, 5312,\n",
       "         5708, 5356, 6657, 8317,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/sizz1997/miniconda3/envs/torch/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(input_ids=encoded['input_ids'], attention_mask=encoded['attention_mask'], stopping_criteria=stopping_criteria)[0]\n",
    "out = tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out[:out.find('</s>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk> 딨 고생을'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
