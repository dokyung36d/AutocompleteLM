{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing, torch, re, tensorboard\n",
    "import os.path as osp\n",
    "import datasets\n",
    "\n",
    "from transformers import EncoderDecoderModel, BertTokenizerFast,\\\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer\n",
    "from tokenization_kobert import KoBertTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from evaluate import load as load_metric\n",
    "from tqdm.auto import tqdm\n",
    "from time import strftime, time, localtime\n",
    "from os import listdir\n",
    "from functools import partial\n",
    "from typing import List\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from utils import rouge, load_dataset, load_iter_augmented_train, load_iter_train_test\n",
    "\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"skt/kogpt2-base-v2\"\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path, \n",
    "#                                                     bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "#                                                     pad_token='<pad>', mask_token='<mask>', max_length=512)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "# model = BertForMaskedLM.from_pretrained(\"skt/kobert-base-v1\")\n",
    "# model = AutoModel.from_pretrained('monologg/distilkobert')\n",
    "# model = BertForMaskedLM.from_pretrained(\"monologg/kobert-lm\")\n",
    "# tokenizer = KoBertTokenizer.from_pretrained('monologg/distilkobert')\n",
    "model = EncoderDecoderModel.from_pretrained(\"kykim/bertshared-kor-base\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"kykim/bertshared-kor-base\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = \"안녕하세요.[MASK][MASK][MASK][MASK][MASK][MASK]\"\n",
    "\n",
    "# collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "# encoded = tokenizer(sent, return_tensors=\"pt\")\n",
    "# target = tokenizer(\"안녕하세요\", return_tensors='pt')\n",
    "\n",
    "# out = model(**encoded)\n",
    "# pipe = FillMaskPipeline(model=model, tokenizer=tokenizer)\n",
    "# while True:\n",
    "#     t = model(**encoded)\n",
    "#     logits:torch.Tensor = t.logits\n",
    "#     predicted_token = logits[:,-1,:].argmax(dim=1)\n",
    "#     print(tokenizer.decode(predicted_token))\n",
    "\n",
    "# print(tokenizer.decode(logits[:,-1,:].argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe(\"안녕하[MASK][MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_idxs = encoded['input_ids'] == tokenizer.mask_token_id\n",
    "# print(mask_idxs)\n",
    "# print(out.logits.shape)\n",
    "# masks = out.logits[mask_idxs].argmax(dim=-1)\n",
    "# print(masks)\n",
    "# print(tokenizer.decode(masks))\n",
    "# predicted_token = out.logits[0,-1].argmax(-1)\n",
    "# tokenizer.decode(predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n",
    "# dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
    "# dataset = dataset.remove_columns([col for col in dataset.column_names if col != 'text'])\n",
    "# dataset = load_dataset(\"bookcorpus\", split=\"train\", streaming=True).with_format('torch')\n",
    "# dataset = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length for the tokenizer is: 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_dir(): return f\"{strftime('%m-%d-%H-%M', localtime(time()))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(tokenizer=tokenizer)\n",
    "trainset, testset = load_iter_train_test(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import load_iter_augmented_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def batch_iterator(batch_size=1):\n",
    "# def batch_iterator():\n",
    "#     dset = iter(load_iter_augmented_train())\n",
    "#     # for i in tqdm(range(0, 1000000, batch_size)):\n",
    "#     for i in tqdm(range(1000000)):\n",
    "#         # yield trainset[i:i+batch_size]['text']\n",
    "#         _item = next(dset)\n",
    "#         # print(_item)\n",
    "#         yield _item['text']\n",
    "# if [_dir for _dir in listdir() if \"tokenizer\" in _dir] != []:\n",
    "#     latest_tokenizer_path = sorted([_dir for _dir in listdir() if \"tokenizer\" in _dir])[-1]\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(latest_tokenizer_path)\n",
    "# else:\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\").train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=32_000)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"skplanet/dialog-koelectra-small-discriminator\").train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=42_000)\n",
    "#     tokenizer.save_pretrained(\"tokenizer_\"+get_time_dir())\n",
    "\n",
    "# tokenizer.pad_token = '[PAD]'\n",
    "# tokenizer.eos_token = '[SEP]'\n",
    "# tokenizer.bos_token = '[SEP]'\n",
    "# tokenizer.unk_token = '[UNK]'\n",
    "# tokenizer.mask_token = '[MASK]'\n",
    "# tokenizer.model_max_length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 10668, 5206, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"안가ㅇ\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##ㅇ'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(5206)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 집가 [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"집가\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 2927, 5571, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"안녀\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one = dataset['train'][0]\n",
    "# one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model.config.max_length = 32\n",
    "model.config.early_stopping = True\n",
    "# model.config.no_repeat_ngram_size = 1\n",
    "model.config.length_penalty = 2.\n",
    "model.config.repetition_penalty = 3.\n",
    "model.config.num_beams = 10\n",
    "model.config.vocab_size = model.config.encoder.vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"output\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=1_000,\n",
    "    logging_steps=1_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    # num_train_epochs=1,\n",
    "    weight_decay=.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=2_000,\n",
    "    fp16=True,\n",
    "    num_train_epochs=5,\n",
    "    save_total_limit=1,\n",
    "    max_steps=100_000,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    compute_metrics=partial(rouge, tokenizer=tokenizer),\n",
    "    train_dataset=trainset,\n",
    "    eval_dataset=testset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(\n",
    "#     \"gpt2\",\n",
    "#     vocab_size = len(tokenizer),\n",
    "#     n_ctx = tokenizer.model_max_length,\n",
    "#     bos_token_id=tokenizer.bos_token_id,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "# model = GPT2LMHeadModel(config)\n",
    "# model_size = sum(t.numel() for t in model.parameters())\n",
    "# print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   218/100000 55:24 < 426:38:14, 0.06 it/s, Epoch 0.00/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
